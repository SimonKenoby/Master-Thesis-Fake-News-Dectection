\chapter{Attention Mechanism}
\section{Introduction}
In this section, we will focus on the deep learning models, the first one being a bidirectional LSTM and the second one an attention layer is added to this LSTM. But it is need to use an other text embedding in order to work with LSTM. Indeed, tf-idf create a sparse matrix with each row corresponding to a value for a given word. This means that the order of the words are lost. In order to solve this, word2vec\cite{Mikolov2013} is used. It allows to match words to continuous vectors of a given size with interesting properties. An other methods, which consist in making word embedding as tuning parameters will be used.

\section{Text to Vectors}
\subsection{Word2Vec}
Word2Vec comes in two fashions: continuous bag of word (CBOW) and skip-gram. It is originaly designed to predic a word given a context. For instance, given two previous words and the two next words, which word is the most likely to take place between them. But it appears that the hidden representation of these words works well as word embedding and have very interesing properties such that words with similar meaning have similar vector representation. It is also possible to perform arithmetic that captures informations such as singular, plural or even capital and countries. For example, we have that $dog - dogs \approx cat - cats$ but also $Paris - France \approx Berlin - Germany$. \\

It is possible to visualize these relasionships by using t-SNE for projecting high dimentions word vectors in 2D space. The results of various relashionships can be see at \textbf{Figure \ref{fig:chap4:word2vec}}.
\begin{figure*}
	\centering
	\includegraphics[width=\textwidth]{images/chapitre4/linear-relationships}
	\caption{Relashionships between different words with t-SNE dimentionality reduciton. }
	\label{fig:chap4:word2vec}
\end{figure*}

\subsubsection{How does it works?} 
As the orignal authors did not inteded this kind of results, Xin Rong\cite{Rong2014} did a good job explaing how it works. 

Let V be the size of the vocabulary and that there is only one word in the CBOW model, it give \textbf{Figure \ref{fig:chap4:CBOW}} model. 

\begin{figure*}
	\centering
	\includegraphics[width=\textwidth]{images/chapitre4/CBOW}
	\caption{A simple CBOW model with only one word in the context}
	\label{fig:chap4:CBOW}
\end{figure*}

Each words is encoded as a one-hot vector of size V. That means that it is a sparse vector full of zeros except for the position assigned to that word which is one. The hidden layers is computed as 
\begin{equation}
	\mathbf{h} = \mathbf{W^Tx}
\end{equation}

Where $\mathbf{W^{V \times N}}$ is the weighte matrix to optimize over. 

The output layer values are computed as 
\begin{equation}
	\mathbf{Y} = \mathbf{W'^Th}
\end{equation}

As before $\mathbf{W'^{N \times V}}$ is also a weight matrix to optimize. The loss can be computed as softmax cross entropy. \\

It is also possible to make the oposite: predicting the context given a single input word. This is the skip-gram model. In this case the loss become \textbf{Equation \ref{eq:loss}}.

\begin{equation}
	E = - \sum_{c=1}^C u_{j^*_c} + C \cdot \sum_{j' = 1}^V \exp(u_{j'}) \ \label{eq:loss}
\end{equation}

$j_c^*$ is the index of the cth output context word and $u_{j^*_c}$ is the score of the jth word in the vocabulary for the cth context word. Finly, the embedding that is used are the value of the hidden layers produced for a given word. 

\begin{figure*}
	\centering
	\includegraphics[width=\textwidth]{images/chapitre4/skip-gram}
	\caption{Skip-gram model with multiple outputs.}
	\label{fig:chap4:skip-gram}
\end{figure*}
\section{LSTM}
LSTM or Long Short Term Memory\cite{Hochreiter1997LongSM} is a kind of reccurent neural network that fits well to temporal or sequential input such as texts. A RNN is a type of neural network where the hidden state is fed in a loop with the sequential inputs. There are usualy shown as unrolled version of it (\textbf{Figure \ref{fig:chap4:RNN_unroll}}). Each of the $X_i$ being one value in the sequence.\\

In this case, $X_i$ values are word vectors. There are two possibilities, either use pre-trained vector with word2vec or make $X_i$ inputs a paramter to learn in the same way as it works for the Word2Vec algorithm, having a one-hot enconding of the word and a matrix of weights to tune. Each methods will be used. \\

Recurrent Neural Networks does not works very well with long-term dependencies, that is why LSTM have been introduced. It is made of an input gate, an output gate and a forget gate that are combined in \textbf{Equation \ref{eq:LSTM}}.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/chapitre4/RNN-unrolled.png}
	\caption{Unrolled RNN (Understanding LSTM Networks, \url{https://colah.github.io/posts/2015-08-Understanding-LSTMs/)}}
	\label{fig:chap4:RNN_unroll}
\end{figure} 

\begin{align} \label{eq:LSTM}
	f_t &= \sigma_g(\mathbf{W}_f x_t + \mathbf{U}_fh_{t-1} + b_f)\\
	i_t &= \sigma_g(\mathbf{W}_i x_t + \mathbf{U}_ih_{t-1} + b_i)\\
	o_t &= \sigma_g(\mathbf{W}_o x_t + \mathbf{U}_oh_{t-1} + b_o)\\
	c_t &= f_t \circ c_{t-1} + i_t \circ \sigma_c(\mathbf{W}_cx_t + \mathbf{U}_c h_{t-1} + b_c)\\
	h_t &= o_t \circ \sigma_h (c_t)
\end{align}

\textbf{Figure \ref{fig:chap4:LSTM-gates}} shows how it works.

\begin{figure}
	\centering
	\includegraphics[width=0.6\textwidth]{images/chapitre4/LSTM1.jpeg}
	\caption{LSTM gates, \\ \url{https://hackernoon.com/understanding-architecture-of-lstm-cell-from-scratch-with-code-8da40f0b71f4)}}
	\label{fig:chap4:LSTM-gates}
\end{figure} 

A bidirectional LSTM works the same way, but the input is fed in the two direction, from the start to the end and from the end to the start.
\section{Attention Mechanism}
Attention mechanism\cite{zhou-etal-2016-attention,Vaswani2017AttentionIA} add an extra layer between LSTM outputs and the final output of the network. It merges word level features into sentence features using a weight vector. \\

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/chapitre4/attention.png}
	\caption{Bidirectional LSTM model with Attention}
	\label{fig:chap4:attention}
\end{figure}

Outputs sequence of the LSTM are summed element-wise in order to merge them. We have that $h_i = [\overrightarrow{h_i} + \overleftarrow{h_i}]$, $\overrightarrow{h_i}$ and $\overleftarrow{h_i}$ begin the outputs i of sequence in each direction as show at \textbf{Figure \ref{fig:chap4:attention}}.\\

Let $H$ be a matrix of the concatenation of all the $h_i$, 
\begin{equation}
	H = [h_1,h_2,...,h_T]
\end{equation}
Where T is the sequence length. 

Then we define 
\begin{align}
	M &= \tanh(H)\\
	\alpha &= softmax(w^TM) \\
	r &= H \alpha^T 
\end{align}
Finaly, we compute $h^* = \tanh(r)$.

For the classification, it uses a softmax classifier as $\hat{p}(y|S) = softmax(W^Sh^* + b)$. Originaly the loss function is the negative log-likelihood, but as in this case it is a binary classification I used binary cross entropy. 
\section{Results}
\subsection{Methodology}
In order to train the models and perform hyper parameters optimization grid search have been used when it was possibile (on the liar-liar dataset) and knwoleadge aquiered there have been used in order to tune parameters for the networks on the \textbf{Fake News Corpus}. In addition, in order to find the bests parameters among all tested with gird search, for each metrics, the training epochs having the highest validation value for that metrics have been choosen. \\

All the models have been trained using adam optimizer and initialized using a normal distribution for the weightes. \\

As SMOTE cannot be used on the \textbf{Fake News Corpus} dues to the size of the corpus, in order to rebalance the dataset the minority class have been over sampled by feeding multiple times the same input by looping through them. 
\subsection{Liar-Liar dataset results}
As explained earlier, both models have been train using different embedding: the first one being pre-trained word2vec vectors of size 300 and the second one beging a tunable parameters with different embedding size.
\subsubsection{LSTM}
When it comes to LSTM trained on liar-liar dataset, it simply does not works. It classify almost all the texts as being from the same class. Althougth, it reaches a good score on the training data, it does not manage to generalize correctly. \textbf{Figure \ref{fig:chap4:lstm2}} shows the recall, precision and f1-score and loss for training and testing set of the best models for the LSTM using word2vec. We can see that even if the training score increase, the testing values ocillates. \\

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/chapitre4/lstm2}
	\caption{Best LSTM with word2vec}
	\label{fig:chap4:lstm2}
\end{figure}

When training the models with word embedding as tunable parameters, the results slightly improves, with an average precision between $55\%$ and $60\%$. This can be seen at \textbf{Figure \ref{fig:chap4:lstm1}}. \\

The training was stopped after 200 iterations because the validation score was not improving anymore. 

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/chapitre4/lstm1}
	\caption{Best LSTM with word embedding as tunable parameters.}
	\label{fig:chap4:lstm1}
\end{figure}


\subsection{Attention Mechanism}
We could think that adding an extra layer to such a bad model would not make any improvement and be useless, but adding an attention layer does improve a little the results. When using word2vec embedding, the best epoch reach up to $62.9595\%$ of average accuracy, which is better than the simple LSTM. Because there are a lot a models with different parameters, it is interesting to look at the distribution of the results for the best epochs by fixing parameters one by one. \textbf{Figure \ref{fig:chap4:att1:confInt}} shows the $95\%$ confidence interval for precision for a fixed parameter. \\

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/chapitre4/confInt_precision_liar_attention_word2vec}
	\caption{Confidence interval of precision for each paramter value}
	\label{fig:chap4:att1:confInt}
\end{figure}

It shows that it is better to use less hidden units in the model, and only a single layer. The sequence length have a very small impact on the precision. Acctualy, the best model uses a sequence length of 20. \\\

The precision of the different models range from $53\%$ to $63\%$ (\textbf{Figure \ref{fig:chap4:att1:distPrecision}}.)

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/chapitre4/distplot_precision_liar_attention_word2vec}
	\caption{Distribution of the precision of best epochs for all the models, trained with word2vec embedding.}
	\label{fig:chap4:att1:distPrecision}
\end{figure}

The training plot of the model that reach the maximum precision can be seen at \textbf{figure \ref{fig:chap4:att1:train}}. It shows that after the 25th iteration, the validation values start to decrease, which is a sign of overfitting. \\

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/chapitre4/attention1}
	\caption{Training and validation of the model with top precision, trained with word2vec embedding.}
	\label{fig:chap4:att1:train}
\end{figure}

Finaly, there is the models where the embedding is a tunable parameter. The \textbf{Figure \ref{fig:chap4:att3:confInt2}} shows that in this case, the longer the sequence the better, and that as before using few hidden units perform better. In this case, variation have a wider range than when using word2vec. 

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/chapitre4/confInt_precision_liar_attention_200}
	\caption{Training and validation of the model with top precision}
	\label{fig:chap4:att3:confInt2}
\end{figure}

There are a few models that have top precision higher than $75\%$, but looking at the training plot (\textbf{Appendix \ref{Appendix2}, Figure \ref{appendix2:training_plot1}}) shows that the model does not perform well. Because in particular case precision in not a good indicator on how well a model perform, f1-score will be used instead, as it a balance between precision and recall. \\

And the best f1-score obtained is $0.55384$, which is quite smaller than the $0.63$ for the model using word2vec. The training plot is at \textbf{Figure \ref{fig:chap4:att3:f1}}. We can see that there is still room for improvement, the next step is to see what happens when training on more epochs. \\

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/chapitre4/attention-f1}
	\caption{Training and validation of the model with top f1-score}
	\label{fig:chap4:att3:f1}
\end{figure}

Training on 1000 epochs rather than 200 does not improves validation score, but it does for training (\textbf{Figure \ref{fig:chap4:att3:f1.1}}). \\

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/chapitre4/attention3}
	\caption{Training on 1000 epochs rather than 200}
	\label{fig:chap4:att3:f1.1}
\end{figure}

\subsection{Result Analysis}
The previous section shows a few things
\begin{itemize}
	\item LSTMs do not work well,
	\item Adding attention layer improve LSTM results,
	\item Using word2vec rather than training the embedding gives better results.
\end{itemize}

It also shows that despite reaching a very good precision, recall and f1-score on the training set it does not perform well on the validation set. This is a sign of overfitting. In order to avoid this multiple methods have been applied without showing any improvement. \\
{}
The following methods have been applied: 
\begin{itemize}
	\item Dropout\cite{srivastava2014dropout}, 
	\item batch-normlaziation\cite{Ioffe2015},
	\item reducing network capacity (less hidden layer, lower embedding dimension, less training paramater with word2vec),
	\item Early stopping of training.
\end{itemize}

The highest gain was from using word2vec embedding. This significantly reduce the number of training parameters, secondly dropout also helped a little. 

\subsection{Testing}

The same way as in \textbf{Chapter \ref{chap3}}, the models will be trained on the parameters that produced the best results on the training set, and trained on training and validation set, and tested on testing set. \\

The parameters used for training are given at \textbf{Table \ref{table:chap4:param}}.

\begin{table}[h]
	\begin{tabular}{|c|ccccc|}
	\hline
	model & embedding size & Sequence Length & num hiddens & droupout & Early Stop\\
	\hline
	LSTM & 300 & 10 & 50 & 0.75 & 126\\
	LSTM + word2vec & 300 & 10 & 50 & 0.0 & 160\\
	Attention & 10 & 20 & 10 & 0.75 & 400\\
	Attention + word2vec & 300 & 20 & 5 & 0.75 & 25\\
	\hline
	\end{tabular}
	\caption{Parameters used for training}
	\label{table:chap4:param}
\end{table}
The results for all four models are given at \textbf{Table \ref{table:chap4:results}}. It shows that the model that works the best is attention network using word2vec embedding, with an accuracy of $61\%$, which is equivalent to ridge classifier and linear svm. The three other models does not perform well, all having a average precision around $55\%$, which is close to being a random classifier. 
\begin{table}
\centering
\begin{subtable}{\textwidth}
\begin{tabular}{lrrrrr}
\toprule
{} &         fake &     reliable &  accuracy &    macro avg &  weighted avg \\
\midrule
f1-score  &     0.440574 &     0.649551 &  0.569061 &     0.545062 &      0.558340 \\
precision &     0.508274 &     0.599526 &  0.569061 &     0.553900 &      0.559698 \\
recall    &     0.388788 &     0.708683 &  0.569061 &     0.548736 &      0.569061 \\
support   &  1106.000000 &  1428.000000 &  0.569061 &  2534.000000 &   2534.000000 \\
\bottomrule
\end{tabular}
\caption{Simple LSTM}
\end{subtable}

\begin{subtable}{\textwidth}
	\begin{tabular}{lrrrrr}
	\toprule
	{} &         fake &     reliable &  accuracy &    macro avg &  weighted avg \\
	\midrule
	f1-score  &     0.481724 &     0.623040 &  0.563536 &     0.552382 &      0.561361 \\
	precision &     0.500000 &     0.606906 &  0.563536 &     0.553453 &      0.560245 \\
	recall    &     0.464738 &     0.640056 &  0.563536 &     0.552397 &      0.563536 \\
	support   &  1106.000000 &  1428.000000 &  0.563536 &  2534.000000 &   2534.000000 \\
	\bottomrule
	\end{tabular}
	\caption{LSTM + word2vec}
\end{subtable}

\begin{subtable}{\textwidth}
\begin{tabular}{lrrrrr}
\toprule
{} &         fake &     reliable &  accuracy &    macro avg &  weighted avg \\
\midrule
f1-score  &     0.486636 &     0.615597 &  0.560379 &     0.551116 &      0.559310 \\
precision &     0.496241 &     0.606803 &  0.560379 &     0.551522 &      0.558546 \\
recall    &     0.477396 &     0.624650 &  0.560379 &     0.551023 &      0.560379 \\
support   &  1106.000000 &  1428.000000 &  0.560379 &  2534.000000 &   2534.000000 \\
\bottomrule
\end{tabular}
\caption{Attention network}
\end{subtable}

\begin{subtable}{\textwidth}
\begin{tabular}{lrrrrr}
\toprule
{} &         fake &     reliable &  accuracy &    macro avg &  weighted avg \\
\midrule
f1-score  &     0.511397 &     0.676721 &  0.610892 &     0.594059 &      0.604563 \\
precision &     0.565789 &     0.636252 &  0.610892 &     0.601021 &      0.605497 \\
recall    &     0.466546 &     0.722689 &  0.610892 &     0.594618 &      0.610892 \\
support   &  1106.000000 &  1428.000000 &  0.610892 &  2534.000000 &   2534.000000 \\
\bottomrule
\end{tabular}
\caption{Attention Network + word2vec}
\end{subtable}
\caption{Results for the differents models trained with parameters given at \textbf{Table \ref{table:chap4:param}}.}
\label{table:chap4:results}
\end{table}