% Encoding: UTF-8

@Article{Zhou2015,
  author      = {Chunting Zhou and Chonglin Sun and Zhiyuan Liu and Francis C. M. Lau},
  title       = {A C-LSTM Neural Network for Text Classification},
  abstract    = {Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling. Convolutional neural network (CNN) and recurrent neural network (RNN) are two mainstream architectures for such modeling tasks, which adopt totally different ways of understanding natural languages. In this work, we combine the strengths of both architectures and propose a novel and unified model called C-LSTM for sentence representation and text classification. C-LSTM utilizes CNN to extract a sequence of higher-level phrase representations, and are fed into a long short-term memory recurrent neural network (LSTM) to obtain the sentence representation. C-LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics. We evaluate the proposed architecture on sentiment classification and question classification tasks. The experimental results show that the C-LSTM outperforms both CNN and LSTM and can achieve excellent performance on these tasks.},
  date        = {2015-11-27},
  eprint      = {1511.08630v2},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1511.08630v2:PDF},
  keywords    = {cs.CL},
}

@Article{Chawla2011,
  author       = {N. V. Chawla and K. W. Bowyer and L. O. Hall and W. P. Kegelmeyer},
  title        = {SMOTE: Synthetic Minority Over-sampling Technique},
  abstract     = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
  date         = {2011-06-09},
  doi          = {10.1613/jair.953},
  eprint       = {1106.1813v1},
  eprintclass  = {cs.AI},
  eprinttype   = {arXiv},
  file         = {online:http\://arxiv.org/pdf/1106.1813v1:PDF},
  journaltitle = {Journal Of Artificial Intelligence Research, Volume 16, pages 321-357, 2002},
  keywords     = {cs.AI},
}

@Article{Rao2016,
  author      = {Adithya Rao and Nemanja Spasojevic},
  title       = {Actionable and Political Text Classification using Word Embeddings and LSTM},
  abstract    = {In this work, we apply word embeddings and neural networks with Long Short-Term Memory (LSTM) to text classification problems, where the classification criteria are decided by the context of the application. We examine two applications in particular. The first is that of Actionability, where we build models to classify social media messages from customers of service providers as Actionable or Non-Actionable. We build models for over 30 different languages for actionability, and most of the models achieve accuracy around 85%, with some reaching over 90% accuracy. We also show that using LSTM neural networks with word embeddings vastly outperform traditional techniques. Second, we explore classification of messages with respect to political leaning, where social media messages are classified as Democratic or Republican. The model is able to classify messages with a high accuracy of 87.57%. As part of our experiments, we vary different hyperparameters of the neural networks, and report the effect of such variation on the accuracy. These actionability models have been deployed to production and help company agents provide customer support by prioritizing which messages to respond to. The model for political leaning has been opened and made available for wider use.},
  date        = {2016-07-08},
  eprint      = {1607.02501v2},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1607.02501v2:PDF},
  keywords    = {cs.CL, cs.IR, 68T50, 92B20},
}

@InProceedings{Allcott2017,
  author    = {Hunt Allcott and Matthew Gentzkow},
  title     = {Social Media and Fake News in the 2016 Election},
  booktitle = {Journal of Economic Perspective},
  year      = {2017},
  volume    = {31},
  number    = {2},
}

@Book{gottfried2016news,
  title     = {News Use Across Social Medial Platforms 2016},
  publisher = {Pew Research Center},
  year      = {2016},
  author    = {Gottfried, Jeffrey and Shearer, Elisa},
}

@Article{silverman2016teens,
  author  = {Silverman, Craig and Alexander, Lawrence},
  title   = {How teens in the Balkans are duping Trump supporters with fake news},
  journal = {Buzzfeed News},
  year    = {2016},
  volume  = {3},
}

@Misc{Szpakowski,
  author       = {Maciej Szpakowski},
  title        = {Fake News Corpus},
  howpublished = {\url{https://github.com/several27/FakeNewsCorpus}},
  note         = {Accessed: 2018-10},
}

@Article{shu2018fakenewsnet,
  author  = {Shu, Kai and Mahudeswaran, Deepak and Wang, Suhang and Lee, Dongwon and Liu, Huan},
  title   = {FakeNewsNet: A Data Repository with News Content, Social Context and Dynamic Information for Studying Fake News on Social Media},
  journal = {arXiv preprint arXiv:1809.01286},
  year    = {2018},
}

@Article{shu2017fake,
  author    = {Shu, Kai and Sliva, Amy and Wang, Suhang and Tang, Jiliang and Liu, Huan},
  title     = {Fake News Detection on Social Media: A Data Mining Perspective},
  journal   = {ACM SIGKDD Explorations Newsletter},
  year      = {2017},
  volume    = {19},
  number    = {1},
  pages     = {22--36},
  publisher = {ACM},
}

@Article{shu2017exploiting,
  author  = {Shu, Kai and Wang, Suhang and Liu, Huan},
  title   = {Exploiting Tri-Relationship for Fake News Detection},
  journal = {arXiv preprint arXiv:1712.07709},
  year    = {2017},
}

@Article{Wang2017,
  author      = {William Yang Wang},
  title       = {"Liar, Liar Pants on Fire": A New Benchmark Dataset for Fake News Detection},
  abstract    = {Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present liar: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate meta-data with text. We show that this hybrid approach can improve a text-only deep learning model.},
  date        = {2017-05-01},
  eprint      = {http://arxiv.org/abs/1705.00648v1},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1705.00648v1:PDF},
  keywords    = {cs.CL, cs.CY},
}

@Article{blei2003latent,
  author  = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  title   = {Latent dirichlet allocation},
  journal = {Journal of machine Learning research},
  year    = {2003},
  volume  = {3},
  number  = {Jan},
  pages   = {993--1022},
}

@Book{BirdKleinLoper09,
  title     = {{Natural Language Processing with Python}},
  publisher = {O'Reilly Media},
  year      = {2009},
  author    = {Steven Bird and Ewan Klein and Edward Loper},
}

@InProceedings{rehurek_lrec,
  author    = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
  title     = {{Software Framework for Topic Modelling with Large Corpora}},
  booktitle = {{Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks}},
  year      = {2010},
  pages     = {45--50},
  address   = {Valletta, Malta},
  month     = May,
  publisher = {ELRA},
  note      = {\url{http://is.muni.cz/publication/884893/en}},
  day       = {22},
  language  = {English},
}

@Article{Perez-Rosas2017,
  author      = {Verónica Pérez-Rosas and Bennett Kleinberg and Alexandra Lefevre and Rada Mihalcea},
  title       = {Automatic Detection of Fake News},
  abstract    = {The proliferation of misleading information in everyday access media outlets such as social media feeds, news blogs, and online newspapers have made it challenging to identify trustworthy news sources, thus increasing the need for computational tools able to provide insights into the reliability of online content. In this paper, we focus on the automatic identification of fake content in online news. Our contribution is twofold. First, we introduce two novel datasets for the task of fake news detection, covering seven different news domains. We describe the collection, annotation, and validation process in detail and present several exploratory analysis on the identification of linguistic differences in fake and legitimate news content. Second, we conduct a set of learning experiments to build accurate fake news detectors. In addition, we provide comparative analyses of the automatic and manual identification of fake news.},
  date        = {2017-08-23},
  eprint      = {1708.07104v1},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1708.07104v1:PDF},
  keywords    = {cs.CL},
}

@Article{Reis2019,
  author    = {Reis, Julio CS and Correia, Andr{\'e} and Murai, Fabr{\'\i}cio and Veloso, Adriano and Benevenuto, Fabr{\'\i}cio and Cambria, Erik},
  title     = {Supervised Learning for Fake News Detection},
  journal   = {IEEE Intelligent Systems},
  year      = {2019},
  volume    = {34},
  number    = {2},
  pages     = {76--81},
  publisher = {IEEE},
}

@Article{Pennebaker2001,
  author  = {Pennebaker, James W and Francis, Martha E and Booth, Roger J},
  title   = {Linguistic inquiry and word count: LIWC 2001},
  journal = {Mahway: Lawrence Erlbaum Associates},
  year    = {2001},
  volume  = {71},
  number  = {2001},
  pages   = {2001},
}

@Article{Hochreiter1997LongSM,
  author  = {Sepp Hochreiter and J{\"u}rgen Schmidhuber},
  title   = {Long Short-Term Memory},
  journal = {Neural Computation},
  year    = {1997},
  volume  = {9},
  pages   = {1735-1780},
}

@InProceedings{Vaswani2017AttentionIA,
  author    = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title     = {Attention Is All You Need},
  booktitle = {NIPS},
  year      = {2017},
}

@Article{Li2018,
  author      = {Shuai Li and Wanqing Li and Chris Cook and Ce Zhu and Yanbo Gao},
  title       = {Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN},
  abstract    = {Recurrent neural networks (RNNs) have been widely used for processing sequential data. However, RNNs are commonly difficult to train due to the well-known gradient vanishing and exploding problems and hard to learn long-term patterns. Long short-term memory (LSTM) and gated recurrent unit (GRU) were developed to address these problems, but the use of hyperbolic tangent and the sigmoid action functions results in gradient decay over layers. Consequently, construction of an efficiently trainable deep network is challenging. In addition, all the neurons in an RNN layer are entangled together and their behaviour is hard to interpret. To address these problems, a new type of RNN, referred to as independently recurrent neural network (IndRNN), is proposed in this paper, where neurons in the same layer are independent of each other and they are connected across layers. We have shown that an IndRNN can be easily regulated to prevent the gradient exploding and vanishing problems while allowing the network to learn long-term dependencies. Moreover, an IndRNN can work with non-saturated activation functions such as relu (rectified linear unit) and be still trained robustly. Multiple IndRNNs can be stacked to construct a network that is deeper than the existing RNNs. Experimental results have shown that the proposed IndRNN is able to process very long sequences (over 5000 time steps), can be used to construct very deep networks (21 layers used in the experiment) and still be trained robustly. Better performances have been achieved on various tasks by using IndRNNs compared with the traditional RNN and LSTM. The code is available at https://github.com/Sunnydreamrain/IndRNN_Theano_Lasagne.},
  date        = {2018-03-13},
  eprint      = {1803.04831v3},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1803.04831v3:PDF},
  keywords    = {cs.CV},
}

@InProceedings{zhou-etal-2016-attention,
  author    = {Zhou, Peng and Shi, Wei and Tian, Jun and Qi, Zhenyu and Li, Bingchen and Hao, Hongwei and Xu, Bo},
  title     = {Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  year      = {2016},
  pages     = {207--212},
  address   = {Berlin, Germany},
  month     = aug,
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/P16-2034},
  url       = {https://www.aclweb.org/anthology/P16-2034},
}

@Article{kim_convolutional_2014,
  author   = {Kim, Yoon},
  title    = {Convolutional {Neural} {Networks} for {Sentence} {Classification}},
  journal  = {arXiv:1408.5882 [cs]},
  year     = {2014},
  month    = aug,
  note     = {arXiv: 1408.5882},
  abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
  file     = {arXiv\:1408.5882 PDF:/home/simon/Zotero/storage/9E97T5N2/Kim - 2014 - Convolutional Neural Networks for Sentence Classif.pdf:application/pdf;arXiv.org Snapshot:/home/simon/Zotero/storage/Y5UBBVXU/1408.html:text/html},
  keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
  url      = {http://arxiv.org/abs/1408.5882},
  urldate  = {2019-07-19},
}

@InProceedings{yang_hierarchical_2016,
  author    = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
  title     = {Hierarchical {Attention} {Networks} for {Document} {Classification}},
  booktitle = {Proceedings of the 2016 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
  year      = {2016},
  pages     = {1480--1489},
  address   = {San Diego, California},
  publisher = {Association for Computational Linguistics},
  abstract  = {We propose a hierarchical attention network for document classiﬁcation. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classiﬁcation tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
  doi       = {10.18653/v1/N16-1174},
  file      = {Yang et al. - 2016 - Hierarchical Attention Networks for Document Class.pdf:/home/simon/Zotero/storage/7W5GMMXJ/Yang et al. - 2016 - Hierarchical Attention Networks for Document Class.pdf:application/pdf},
  language  = {en},
  url       = {http://aclweb.org/anthology/N16-1174},
  urldate   = {2019-07-19},
}

@Article{miyato_adversarial_2016,
  author   = {Miyato, Takeru and Dai, Andrew M. and Goodfellow, Ian},
  title    = {Adversarial {Training} {Methods} for {Semi}-{Supervised} {Text} {Classification}},
  journal  = {arXiv:1605.07725 [cs, stat]},
  year     = {2016},
  month    = may,
  note     = {arXiv: 1605.07725},
  abstract = {Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.},
  file     = {arXiv\:1605.07725 PDF:/home/simon/Zotero/storage/CFZ3AD3L/Miyato et al. - 2016 - Adversarial Training Methods for Semi-Supervised T.pdf:application/pdf;arXiv.org Snapshot:/home/simon/Zotero/storage/XFTY2GNE/1605.html:text/html},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  url      = {http://arxiv.org/abs/1605.07725},
  urldate  = {2019-07-19},
}

@Article{kowsari_rmdl:_2018,
  author     = {Kowsari, Kamran and Heidarysafa, Mojtaba and Brown, Donald E. and Meimandi, Kiana Jafari and Barnes, Laura E.},
  title      = {{RMDL}: {Random} {Multimodel} {Deep} {Learning} for {Classification}},
  journal    = {Proceedings of the 2nd International Conference on Information System and Data Mining - ICISDM '18},
  year       = {2018},
  pages      = {19--28},
  note       = {arXiv: 1805.01890},
  abstract   = {The continually increasing number of complex datasets each year necessitates ever improving machine learning methods for robust and accurate categorization of these data. This paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble, deep learning approach for classification. Deep learning models have achieved state-of-the-art results across many domains. RMDL solves the problem of finding the best deep learning structure and architecture while simultaneously improving robustness and accuracy through ensembles of deep learning architectures. RDML can accept as input a variety data to include text, video, images, and symbolic. This paper describes RMDL and shows test results for image and text data including MNIST, CIFAR-10, WOS, Reuters, IMDB, and 20newsgroup. These test results show that RDML produces consistently better performance than standard methods over a broad range of data types and classification problems.},
  doi        = {10.1145/3206098.3206111},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
  shorttitle = {{RMDL}},
  url        = {http://arxiv.org/abs/1805.01890},
  urldate    = {2019-07-19},
}

@Article{Maaten2008,
  author   = {Laurens van der Maaten and Geoffrey Hinton},
  title    = {Visualizing Data using t-SNE},
  journal  = {Journal of Machine Learning Research},
  year     = {2008},
  number   = {9},
  pages    = {2579-2605},
  abstract = {We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each
datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic
Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces
significantly better visualizations by reducing the tendency to crowd points together in the center
of the map. t-SNE is better than existing techniques at creating a single map that reveals structure
at many different scales. This is particularly important for high-dimensional data that lie on several
different, but related, low-dimensional manifolds, such as images of objects from multiple classes
seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how
t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the
data to influence the way in which a subset of the data is displayed. We illustrate the performance of
t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization
techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza-
tions produced by t-SNE are significantly better than those produced by the other techniques on
almost all of the data sets.},
  keywords = {visualization, dimensionality reduction, manifold learning, embedding algorithms, multidimensional scaling},
}

@Article{scikit-learn,
  author  = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  title   = {Scikit-learn: Machine Learning in {P}ython},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  pages   = {2825--2830},
}

@Article{Jones2004,
  author    = {Jones, Karen Sp{\"a}rck},
  title     = {A statistical interpretation of term specificity and its application in retrieval},
  journal   = {Journal of documentation},
  year      = {2004},
  publisher = {Emerald Group Publishing Limited},
}

@Misc{Robertson2004,
  author   = {Robertson, Stephen},
  title    = {Understanding inverse document frequency: On theoretical arguments for IDF},
  year     = {2004},
  abstract = {The term weighting function known as IDF was proposed in 1972, and has since been extremely widely used, usually as part of a TF*IDF function. It is often described as a heuristic, and many papers have been written (some based on Shannon’s Information Theory) seeking to establish some theoretical basis for it. Some of these attempts are reviewed, and it is shown that the Information Theory approaches are problematic, but that there are good theoretical justifications of both IDF and TF*IDF in traditional probabilistic model of information retrieval.},
}

@Article{Fan2008LIBLINEARAL,
  author  = {Rong-En Fan and Kai-Wei Chang and Cho-Jui Hsieh and Xiang-Rui Wang and Chih-Jen Lin},
  title   = {LIBLINEAR: A Library for Large Linear Classification},
  journal = {J. Mach. Learn. Res.},
  year    = {2008},
  volume  = {9},
  pages   = {1871-1874},
}

@Article{zhang_optimality_nodate,
  author   = {Zhang, Harry},
  title    = {The {Optimality} of {Naive} {Bayes}},
  pages    = {6},
  abstract = {Naive Bayes is one of the most efﬁcient and effective inductive learning algorithms for machine learning and data mining. Its competitive performance in classiﬁcation is surprising, because the conditional independence assumption on which it is based, is rarely true in realworld applications. An open question is: what is the true reason for the surprisingly good performance of naive Bayes in classiﬁcation? In this paper, we propose a novel explanation on the superb classiﬁcation performance of naive Bayes. We show that, essentially, the dependence distribution; i.e., how the local dependence of a node distributes in each class, evenly or unevenly, and how the local dependencies of all nodes work together, consistently (supporting a certain classiﬁcation) or inconsistently (canceling each other out), plays a crucial role. Therefore, no matter how strong the dependences among attributes are, naive Bayes can still be optimal if the dependences distribute evenly in classes, or if the dependences cancel each other out. We propose and prove a sufﬁcient and necessary conditions for the optimality of naive Bayes. Further, we investigate the optimality of naive Bayes under the Gaussian distribution. We present and prove a sufﬁcient condition for the optimality of naive Bayes, in which the dependence between attributes do exist. This provides evidence that dependence among attributes may cancel out each other. In addition, we explore when naive Bayes works well.},
  file     = {Zhang - The Optimality of Naive Bayes.pdf:/home/simon/Zotero/storage/MFZHYEWX/Zhang - The Optimality of Naive Bayes.pdf:application/pdf},
  language = {en},
}

@Article{Mikolov2013,
  author      = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  title       = {Efficient Estimation of Word Representations in Vector Space},
  abstract    = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  date        = {2013-01-16},
  eprint      = {http://arxiv.org/abs/1301.3781v3},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1301.3781v3:PDF},
  keywords    = {cs.CL},
}

@Article{Rong2014,
  author      = {Xin Rong},
  title       = {word2vec Parameter Learning Explained},
  abstract    = {The word2vec model and application by Mikolov et al. have attracted a great amount of attention in recent two years. The vector representations of words learned by word2vec models have been shown to carry semantic meanings and are useful in various NLP tasks. As an increasing number of researchers would like to experiment with word2vec or similar techniques, I notice that there lacks a material that comprehensively explains the parameter learning process of word embedding models in details, thus preventing researchers that are non-experts in neural networks from understanding the working mechanism of such models. This note provides detailed derivations and explanations of the parameter update equations of the word2vec models, including the original continuous bag-of-word (CBOW) and skip-gram (SG) models, as well as advanced optimization techniques, including hierarchical softmax and negative sampling. Intuitive interpretations of the gradient equations are also provided alongside mathematical derivations. In the appendix, a review on the basics of neuron networks and backpropagation is provided. I also created an interactive demo, wevi, to facilitate the intuitive understanding of the model.},
  date        = {2014-11-11},
  eprint      = {http://arxiv.org/abs/1411.2738v4},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1411.2738v4:PDF},
  keywords    = {cs.CL},
}

@InProceedings{Wang2018,
  author       = {Wang, Yaqing and Ma, Fenglong and Jin, Zhiwei and Yuan, Ye and Xun, Guangxu and Jha, Kishlay and Su, Lu and Gao, Jing},
  title        = {Eann: Event adversarial neural networks for multi-modal fake news detection},
  booktitle    = {Proceedings of the 24th acm sigkdd international conference on knowledge discovery \& data mining},
  year         = {2018},
  pages        = {849--857},
  organization = {ACM},
}

@InProceedings{Thorne2017,
  author    = {Thorne, James and Chen, Mingjie and Myrianthous, Giorgos and Pu, Jiashu and Wang, Xiaoxuan and Vlachos, Andreas},
  title     = {Fake news stance detection using stacked ensemble of classifiers},
  booktitle = {Proceedings of the 2017 EMNLP Workshop: Natural Language Processing meets Journalism},
  year      = {2017},
  pages     = {80--83},
}

@InProceedings{Ruchansky2017,
  author       = {Ruchansky, Natali and Seo, Sungyong and Liu, Yan},
  title        = {Csi: A hybrid deep model for fake news detection},
  booktitle    = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
  year         = {2017},
  pages        = {797--806},
  organization = {ACM},
}

@InProceedings{Long2017,
  author    = {Long, Yunfei and Lu, Qin and Xiang, Rong and Li, Minglei and Huang, Chu-Ren},
  title     = {Fake news detection through multi-perspective speaker profiles},
  booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
  year      = {2017},
  pages     = {252--256},
}

@InProceedings{Granik2017,
  author       = {Granik, Mykhailo and Mesyura, Volodymyr},
  title        = {Fake news detection using naive Bayes classifier},
  booktitle    = {2017 IEEE First Ukraine Conference on Electrical and Computer Engineering (UKRCON)},
  year         = {2017},
  pages        = {900--903},
  organization = {IEEE},
}

@Article{Yang2018,
  author      = {Yang Yang and Lei Zheng and Jiawei Zhang and Qingcai Cui and Zhoujun Li and Philip S. Yu},
  title       = {TI-CNN: Convolutional Neural Networks for Fake News Detection},
  abstract    = {With the development of social networks, fake news for various commercial and political purposes has been appearing in large numbers and gotten widespread in the online world. With deceptive words, people can get infected by the fake news very easily and will share them without any fact-checking. For instance, during the 2016 US president election, various kinds of fake news about the candidates widely spread through both official news media and the online social networks. These fake news is usually released to either smear the opponents or support the candidate on their side. The erroneous information in the fake news is usually written to motivate the voters' irrational emotion and enthusiasm. Such kinds of fake news sometimes can bring about devastating effects, and an important goal in improving the credibility of online social networks is to identify the fake news timely. In this paper, we propose to study the fake news detection problem. Automatic fake news identification is extremely hard, since pure model based fact-checking for news is still an open problem, and few existing models can be applied to solve the problem. With a thorough investigation of a fake news data, lots of useful explicit features are identified from both the text words and images used in the fake news. Besides the explicit features, there also exist some hidden patterns in the words and images used in fake news, which can be captured with a set of latent features extracted via the multiple convolutional layers in our model. A model named as TI-CNN (Text and Image information based Convolutinal Neural Network) is proposed in this paper. By projecting the explicit and latent features into a unified feature space, TI-CNN is trained with both the text and image information simultaneously. Extensive experiments carried on the real-world fake news datasets have demonstrate the effectiveness of TI-CNN.},
  date        = {2018-06-03},
  eprint      = {1806.00749v1},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1806.00749v1:PDF},
  keywords    = {cs.CL, cs.SI},
}

@Article{Tacchini2017,
  author       = {Eugenio Tacchini and Gabriele Ballarin and Marco L. Della Vedova and Stefano Moret and Luca de Alfaro},
  title        = {Some Like it Hoax: Automated Fake News Detection in Social Networks},
  abstract     = {In recent years, the reliability of information on the Internet has emerged as a crucial issue of modern society. Social network sites (SNSs) have revolutionized the way in which information is spread by allowing users to freely share content. As a consequence, SNSs are also increasingly used as vectors for the diffusion of misinformation and hoaxes. The amount of disseminated information and the rapidity of its diffusion make it practically impossible to assess reliability in a timely manner, highlighting the need for automatic hoax detection systems. As a contribution towards this objective, we show that Facebook posts can be classified with high accuracy as hoaxes or non-hoaxes on the basis of the users who "liked" them. We present two classification techniques, one based on logistic regression, the other on a novel adaptation of boolean crowdsourcing algorithms. On a dataset consisting of 15,500 Facebook posts and 909,236 users, we obtain classification accuracies exceeding 99% even when the training set contains less than 1% of the posts. We further show that our techniques are robust: they work even when we restrict our attention to the users who like both hoax and non-hoax posts. These results suggest that mapping the diffusion pattern of information can be a useful component of automatic hoax detection systems.},
  date         = {2017-04-25},
  eprint       = {1704.07506v1},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  file         = {online:http\://arxiv.org/pdf/1704.07506v1:PDF},
  journaltitle = {Proceedings of the Second Workshop on Data Science for Social Good (SoGood), Skopje, Macedonia, 2017. CEUR Workshop Proceedings Volume 1960, 2017},
  keywords     = {cs.LG, cs.HC, cs.SI},
}

@Book{Rokach:2014:DMD:2755359,
  title     = {Data Mining With Decision Trees: Theory and Applications},
  publisher = {World Scientific Publishing Co., Inc.},
  year      = {2014},
  author    = {Rokach, Lior and Maimon, Oded},
  address   = {River Edge, NJ, USA},
  edition   = {2nd},
  isbn      = {9789814590075, 981459007X},
}

@Misc{WWF2019,
  author = {WWF},
  title  = {WWF 10YearsChallenge},
  year   = {2019},
  url    = {https://www.pausecafein.fr/culture/culture-internet-photographie-virale-fake.html},
}

@Article{Ioffe2015,
  author        = {Sergey Ioffe and Christian Szegedy},
  title         = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  __markedentry = {[simon:]},
  abstract      = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.},
  date          = {2015-02-11},
  eprint        = {1502.03167v3},
  eprintclass   = {cs.LG},
  eprinttype    = {arXiv},
  file          = {online:http\://arxiv.org/pdf/1502.03167v3:PDF},
  keywords      = {cs.LG},
}

@Article{srivastava2014dropout,
  author    = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  title     = {Dropout: a simple way to prevent neural networks from overfitting},
  journal   = {The journal of machine learning research},
  year      = {2014},
  volume    = {15},
  number    = {1},
  pages     = {1929--1958},
  publisher = {JMLR. org},
}

@Article{Le2014,
  author        = {Quoc V. Le and Tomas Mikolov},
  title         = {Distributed Representations of Sentences and Documents},
  __markedentry = {[simon:6]},
  abstract      = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
  date          = {2014-05-16},
  eprint        = {1405.4053v2},
  eprintclass   = {cs.CL},
  eprinttype    = {arXiv},
  file          = {online:http\://arxiv.org/pdf/1405.4053v2:PDF},
  keywords      = {cs.CL, cs.AI, cs.LG},
}

@InCollection{NIPS2011_4396,
  author    = {David R. Karger and Oh, Sewoong and Shah, Devavrat},
  title     = {Iterative Learning for Reliable Crowdsourcing Systems},
  booktitle = {Advances in Neural Information Processing Systems 24},
  publisher = {Curran Associates, Inc.},
  year      = {2011},
  editor    = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
  pages     = {1953--1961},
  url       = {http://papers.nips.cc/paper/4396-iterative-learning-for-reliable-crowdsourcing-systems.pdf},
}

@Comment{jabref-meta: databaseType:bibtex;}
